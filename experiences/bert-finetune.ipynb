{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c27818-9b42-4e34-b8cc-c27f435d017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd6b0cd-389d-42e1-bc2b-5f15d1e2e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "EPOCH_NUMBERS = 10\n",
    "BATCH_SIZE = 16\n",
    "WIQA_TRAIN = '../datasets/wiqa-dataset-v2-october-2019/train.jsonl'\n",
    "WIQA_TEST = '../datasets/wiqa-dataset-v2-october-2019/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19843d35-9940-49d5-9696-b1fc9935d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d7ff20-f205-41fb-8510-360ff47c916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WIQA_TRAIN, 'r') as json_file:\n",
    "    train_data = [json.loads(line) for line in json_file]\n",
    "\n",
    "with open(WIQA_TEST, 'r') as json_file:\n",
    "    test_data = [json.loads(line) for line in json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e5255e-a7f0-4577-8b58-82614b72bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 3, # The number of output labels. Here, it's 3: more, less, no effect.\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "\n",
    "# Extract context (para_steps), questions (stem), and labels (answer_label)\n",
    "contexts = [\". \".join(item['question']['para_steps']) for item in train_data]\n",
    "questions = [item['question']['stem'][:-1] + \"?\" for item in train_data]\n",
    "labels = [item['question']['answer_label'] for item in train_data]\n",
    "\n",
    "# Convert labels to numerical form (\"more\": 0, \"less\": 1, \"no effect\": 2)\n",
    "label_dict = {\"more\": 0, \"less\": 1, \"no_effect\": 2}\n",
    "labels = [label_dict[label] for label in labels]\n",
    "\n",
    "# Split your data into train and validation sets\n",
    "train_contexts, val_contexts, train_questions, val_questions, train_labels, val_labels = train_test_split(contexts, questions, labels, test_size=.2)\n",
    "\n",
    "# Tokenize the context and questions\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
    "\n",
    "# Convert to PyTorch data types\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_labels))\n",
    "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_labels))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0053f9f7-db26-4541-9e50-f0b1077f0e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0, Count: 9936\n",
      "Label: 2, Count: 9936\n",
      "Label: 1, Count: 9936\n",
      "As you can see we have balanced classes so we do not need class weighting\n"
     ]
    }
   ],
   "source": [
    "# Count the number of instances for each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Print the counts\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label: {label}, Count: {count}\")\n",
    "print(\"As you can see we have balanced classes so we do not need class weighting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2c1b56-8a1f-4572-91e6-ddcfaf96b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6585332d-1fa8-49b0-9a96-04cc71f76184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  1.1616610288619995\n",
      "Epoch: 1, Validation Loss: 1.1415713012378272\n",
      "Epoch: 2, Loss:  0.49950483441352844\n",
      "Epoch: 2, Validation Loss: 0.6087750989054866\n",
      "Epoch: 3, Loss:  0.41587209701538086\n",
      "Epoch: 3, Validation Loss: 0.5827380743966345\n",
      "Epoch: 4, Loss:  0.40718644857406616\n",
      "Epoch: 4, Validation Loss: 0.5942742222915066\n",
      "Epoch: 5, Loss:  0.4850940406322479\n",
      "Epoch: 5, Validation Loss: 0.5939471165552216\n",
      "Epoch: 6, Loss:  0.33411142230033875\n",
      "Epoch: 6, Validation Loss: 0.6087316660555054\n",
      "Epoch: 7, Loss:  0.4086441099643707\n",
      "Epoch: 7, Validation Loss: 0.6254913878025382\n",
      "Epoch: 8, Loss:  0.5053379535675049\n",
      "Epoch: 8, Validation Loss: 0.6316750355204692\n",
      "Epoch: 9, Loss:  0.6028084754943848\n",
      "Epoch: 9, Validation Loss: 0.6278014159873727\n",
      "Epoch: 10, Loss:  0.3517250120639801\n",
      "Epoch: 10, Validation Loss: 0.60191412437857\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define the training function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(train_loader, 0):\n",
    "        ids = data[0].to(device, dtype = torch.long)\n",
    "        mask = data[1].to(device, dtype = torch.long)\n",
    "        targets = data[2].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, labels = targets)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "            \n",
    "        # Validation step\n",
    "        if _ % 5000 == 0:  # adjust this to run validation every N steps\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for val_data in val_loader:\n",
    "                val_ids = val_data[0].to(device, dtype = torch.long)\n",
    "                val_mask = val_data[1].to(device, dtype = torch.long)\n",
    "                val_targets = val_data[2].to(device, dtype = torch.long)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(val_ids, val_mask, labels=val_targets)\n",
    "                    val_loss = outputs[0]\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            print(f'Epoch: {epoch}, Validation Loss: {avg_val_loss}')\n",
    "                    \n",
    "\n",
    "            \n",
    "            # Switch back to train mode\n",
    "            model.train()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, EPOCH_NUMBERS+1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdadabd6-c5e0-4839-b608-52f918cc3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(para_steps, question):\n",
    "    # Join the steps into a single text\n",
    "    context = ' '.join(para_steps)\n",
    "    \n",
    "    # Prepare the inputs for the model\n",
    "    inputs = tokenizer(context, question, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "    # Move the inputs to the device\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    # Get the model's predictions\n",
    "    output = model(**inputs)\n",
    "\n",
    "    _, prediction = torch.max(output.logits, dim=1)\n",
    "\n",
    "    # Convert numerical prediction back to original label\n",
    "    reverse_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    return reverse_label_dict[prediction.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455843a-573c-480b-a162-16d24da9d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3003/3003 [00:35<00:00, 85.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# List to store the true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Map text to corresponding label\n",
    "text_to_label = {'more': 0, 'less': 1, 'no_effect': 2}\n",
    "\n",
    "for entry in tqdm(test_data):\n",
    "    question = entry['question']['stem']\n",
    "    para_steps = entry['question']['para_steps']\n",
    "    true_answer = entry['question']['answer_label']\n",
    "    predicted_answer = predict(para_steps, question)\n",
    "    \n",
    "    y_true.append(true_answer)\n",
    "    y_pred.append(predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b83aeb-482e-45c0-9d57-963ccb18b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6753108120784266\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e917a5-7cd8-4ccb-ad34-6de8ba28182a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fulgid",
   "language": "python",
   "name": "fulgid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
