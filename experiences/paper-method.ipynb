{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c27818-9b42-4e34-b8cc-c27f435d017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoModel\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd6b0cd-389d-42e1-bc2b-5f15d1e2e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "EPOCH_NUMBERS = 10\n",
    "BATCH_SIZE = 16\n",
    "WIQA_TRAIN = '../datasets/wiqa-dataset-v2-october-2019/train.jsonl'\n",
    "WIQA_DEV = '../datasets/wiqa-dataset-v2-october-2019/dev.jsonl'\n",
    "WIQA_TEST = '../datasets/wiqa-dataset-v2-october-2019/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19843d35-9940-49d5-9696-b1fc9935d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d7ff20-f205-41fb-8510-360ff47c916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WIQA_TRAIN, 'r') as json_file:\n",
    "    train_data = [json.loads(line) for line in json_file]\n",
    "    \n",
    "with open(WIQA_DEV, 'r') as json_file:\n",
    "    dev_data = [json.loads(line) for line in json_file]\n",
    "\n",
    "with open(WIQA_TEST, 'r') as json_file:\n",
    "    test_data = [json.loads(line) for line in json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e5255e-a7f0-4577-8b58-82614b72bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/kuacc/users/ebarkhordar23/.conda/envs/fulgid/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 3,  # The number of output labels: \"more\", \"less\", \"no_effect\".\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "# Define a mapping for answer options\n",
    "options = [\"more\", \"less\", \"no_effect\"]\n",
    "label_dict = {option: i for i, option in enumerate(options)}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_dict):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_dict = label_dict\n",
    "\n",
    "        for item in data:\n",
    "            context = \" \".join([p.strip() for p in item[\"question\"][\"para_steps\"] if len(p) > 0])\n",
    "            question = item['question']['stem'].strip()\n",
    "            label = item['question']['answer_label'].strip()\n",
    "\n",
    "            for option in options:\n",
    "                encoding = self.tokenizer(context, f\"{question} {option}\", truncation=True, padding='max_length', max_length=512)\n",
    "                self.examples.append({\n",
    "                    \"input_ids\": encoding[\"input_ids\"],\n",
    "                    \"attention_mask\": encoding[\"attention_mask\"],\n",
    "                    \"label\": self.label_dict[label] if label == option else self.label_dict['no_effect']\n",
    "                })\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(self.examples[idx][\"label\"])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "\n",
    "# Assuming your original data is stored in `data`\n",
    "# train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer, label_dict)\n",
    "dev_dataset = CustomDataset(dev_data, tokenizer, label_dict)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16)\n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2c1b56-8a1f-4572-91e6-ddcfaf96b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f0eadf-3092-4649-bb60-99e94817b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save your models\n",
    "save_directory = 'saved_models'\n",
    "\n",
    "# Make sure the directory exists\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6585332d-1fa8-49b0-9a96-04cc71f76184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer (Adam is a common choice)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define the training function\n",
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress = tqdm(data_loader, desc='Training', position=0, leave=True)\n",
    "    \n",
    "    for batch in progress:\n",
    "        # Get the inputs and labels\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Get the loss from the outputs\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Define the validation function\n",
    "def validate_epoch(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    progress = tqdm(data_loader, desc='Validating', position=0, leave=True)\n",
    "    \n",
    "    for batch in progress:\n",
    "        # Get the inputs and labels\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass with no gradient computation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Get the loss from the outputs\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        total_accuracy += (preds == labels).sum().item()\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress.set_postfix({'validation_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "    return total_loss / len(data_loader), total_accuracy / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd9d0d0-fc45-49ac-b2f2-5229de4a1b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model not found. Training a new model.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5589/5589 [32:57<00:00,  2.83it/s, training_loss=0.154]\n",
      "Validating: 100%|██████████| 1293/1293 [02:30<00:00,  8.59it/s, validation_loss=0.050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.383632930385915\n",
      "Validation loss: 0.3764258969483961\n",
      "Validation accuracy: 0.7710569577410309\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5589/5589 [32:54<00:00,  2.83it/s, training_loss=0.123]\n",
      "Validating: 100%|██████████| 1293/1293 [02:30<00:00,  8.60it/s, validation_loss=0.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3439787210045773\n",
      "Validation loss: 0.3696134852113819\n",
      "Validation accuracy: 0.7635141669084228\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5589/5589 [32:54<00:00,  2.83it/s, training_loss=0.067]\n",
      "Validating: 100%|██████████| 1293/1293 [02:30<00:00,  8.60it/s, validation_loss=0.040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3348071646091283\n",
      "Validation loss: 0.3606078447104192\n",
      "Validation accuracy: 0.7778261290010637\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5589/5589 [32:54<00:00,  2.83it/s, training_loss=0.132]\n",
      "Validating: 100%|██████████| 1293/1293 [02:30<00:00,  8.60it/s, validation_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.31097746046896463\n",
      "Validation loss: 0.37403982880612446\n",
      "Validation accuracy: 0.829803694033459\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5589/5589 [32:56<00:00,  2.83it/s, training_loss=0.030]\n",
      "Validating: 100%|██████████| 1293/1293 [02:30<00:00,  8.60it/s, validation_loss=0.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.20200832101375119\n",
      "Validation loss: 0.38699030586588934\n",
      "Validation accuracy: 0.8503046127067014\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5589/5589 [32:56<00:00,  2.83it/s, training_loss=0.002]\n",
      "Validating: 100%|██████████| 1293/1293 [02:30<00:00,  8.60it/s, validation_loss=0.001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1255892313758785\n",
      "Validation loss: 0.47013822328482957\n",
      "Validation accuracy: 0.8475002417561164\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping criteria\n",
    "n_no_improve = 0\n",
    "early_stop_after_n_epochs = 3\n",
    "best_loss = float('inf')\n",
    "\n",
    "model_path = os.path.join(save_directory, f'best_model')\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f'Loading model from {model_path}')\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "else:\n",
    "    print('Pretrained model not found. Training a new model.')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCH_NUMBERS):\n",
    "        print(f'Epoch {epoch+1}/{EPOCH_NUMBERS}')\n",
    "        train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        dev_loss, dev_accuracy = validate_epoch(model, dev_loader)\n",
    "        print(f'Training loss: {train_loss}')\n",
    "        print(f'Validation loss: {dev_loss}')\n",
    "        print(f'Validation accuracy: {dev_accuracy}')\n",
    "\n",
    "        # Check for loss improvement\n",
    "        if dev_loss < best_loss:\n",
    "            best_loss = dev_loss\n",
    "            n_no_improve = 0\n",
    "            model.save_pretrained(model_path)\n",
    "        else:\n",
    "            n_no_improve += 1\n",
    "\n",
    "        # If the validation loss hasn't improved for early_stop_after_n_epochs epochs, stop training\n",
    "        if n_no_improve >= early_stop_after_n_epochs:\n",
    "            print('Early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdadabd6-c5e0-4839-b608-52f918cc3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(para_steps, question):\n",
    "    # Join the steps into a single text\n",
    "    context = ' '.join(para_steps)\n",
    "    \n",
    "    # Prepare the inputs for the model\n",
    "    inputs = tokenizer(context, question, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "    # Move the inputs to the device\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    # Get the model's predictions\n",
    "    output = model(**inputs)\n",
    "\n",
    "    _, prediction = torch.max(output.logits, dim=1)\n",
    "\n",
    "    # Convert numerical prediction back to original label\n",
    "    reverse_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    return reverse_label_dict[prediction.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b455843a-573c-480b-a162-16d24da9d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3003/3003 [00:33<00:00, 89.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# List to store the true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Map text to corresponding label\n",
    "label_dict = {\"more\": 0, \"less\": 1, \"no_effect\": 2}\n",
    "\n",
    "\n",
    "for entry in tqdm(test_data):\n",
    "    question = entry['question']['stem']\n",
    "    para_steps = entry['question']['para_steps']\n",
    "    true_answer = entry['question']['answer_label']\n",
    "    predicted_answer = predict(para_steps, question)\n",
    "    \n",
    "    y_true.append(true_answer)\n",
    "    y_pred.append(predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b83aeb-482e-45c0-9d57-963ccb18b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.71407537422356023\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f'F1 score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fulgid",
   "language": "python",
   "name": "fulgid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
